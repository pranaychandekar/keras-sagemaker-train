{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training Job \n",
    "\n",
    "### Please go through this notebook only if you have finished Part 1 to Part 4 of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 1: Import packages, get IAM role, get the region and set the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket ='keras-sagemaker-train' # Put your s3 bucket name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 2: Create the algorithm image and push to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\r\n",
      "Starting docker:\t.[  OK  ]\r\n",
      "Sending build context to Docker daemon  11.47MB\r",
      "\r\n",
      "Step 1/6 : FROM phenompeople/centos-python:3.6.3\n",
      "3.6.3: Pulling from phenompeople/centos-python\n",
      "7dc0dca2b151: Pulling fs layer\n",
      "4e13d20dd920: Pulling fs layer\n",
      "4e03286a7322: Pulling fs layer\n",
      "1460c005753b: Pulling fs layer\n",
      "b6d4fd0c5aa4: Pulling fs layer\n",
      "b291b7062d5c: Pulling fs layer\n",
      "1460c005753b: Waiting\n",
      "b6d4fd0c5aa4: Waiting\n",
      "b291b7062d5c: Waiting\n",
      "4e03286a7322: Verifying Checksum\n",
      "4e03286a7322: Download complete\n",
      "4e13d20dd920: Verifying Checksum\n",
      "4e13d20dd920: Download complete\n",
      "7dc0dca2b151: Verifying Checksum\n",
      "7dc0dca2b151: Download complete\n",
      "1460c005753b: Verifying Checksum\n",
      "1460c005753b: Download complete\n",
      "b291b7062d5c: Verifying Checksum\n",
      "b291b7062d5c: Download complete\n",
      "b6d4fd0c5aa4: Verifying Checksum\n",
      "b6d4fd0c5aa4: Download complete\n",
      "7dc0dca2b151: Pull complete\n",
      "4e13d20dd920: Pull complete\n",
      "4e03286a7322: Pull complete\n",
      "1460c005753b: Pull complete\n",
      "b6d4fd0c5aa4: Pull complete\n",
      "b291b7062d5c: Pull complete\n",
      "Digest: sha256:860522cc90cb56ce2c033153dcc7ec6079189972dbbe6c28c46fb420692b038e\n",
      "Status: Downloaded newer image for phenompeople/centos-python:3.6.3\n",
      " ---> e3d7d8ca4a30\n",
      "Step 2/6 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Running in b88276ce5458\n",
      "Removing intermediate container b88276ce5458\n",
      " ---> 470f34e723ba\n",
      "Step 3/6 : ADD requirements-cpu.txt /\n",
      " ---> a5699ea2a40c\n",
      "Step 4/6 : RUN pip3 install -r requirements-cpu.txt\n",
      " ---> Running in 34cddd85d6f6\n",
      "Collecting absl-py==0.7.1 (from -r requirements-cpu.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting astor==0.8.0 (from -r requirements-cpu.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting gast==0.2.2 (from -r requirements-cpu.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting grpcio==1.21.1 (from -r requirements-cpu.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/99/83/18f374294bf34128a448ee2fae37651f943b0b5fa473b5b3aff262c15bf8/grpcio-1.21.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "Collecting h5py==2.9.0 (from -r requirements-cpu.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
      "Collecting Keras==2.2.4 (from -r requirements-cpu.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Collecting Keras-Applications==1.0.8 (from -r requirements-cpu.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Collecting Keras-Preprocessing==1.1.0 (from -r requirements-cpu.txt (line 8))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting Markdown==3.1.1 (from -r requirements-cpu.txt (line 9))\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Collecting mock==3.0.5 (from -r requirements-cpu.txt (line 10))\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting numpy==1.16.4 (from -r requirements-cpu.txt (line 11))\n",
      "  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "Collecting protobuf==3.8.0 (from -r requirements-cpu.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/fb/29de8d08967f0cce1bb10b39846d836b0f3bf6776ddc36aed7c73498ca7e/protobuf-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "Collecting PyYAML==5.1.1 (from -r requirements-cpu.txt (line 13))\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
      "Collecting scipy==1.3.0 (from -r requirements-cpu.txt (line 14))\n",
      "  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Collecting six==1.12.0 (from -r requirements-cpu.txt (line 15))\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard==1.13.1 (from -r requirements-cpu.txt (line 16))\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "Collecting tensorflow==1.13.1 (from -r requirements-cpu.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
      "Collecting tensorflow-estimator==1.13.0 (from -r requirements-cpu.txt (line 18))\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "Collecting termcolor==1.1.0 (from -r requirements-cpu.txt (line 19))\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting Werkzeug==0.15.4 (from -r requirements-cpu.txt (line 20))\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl (327kB)\n",
      "Collecting setuptools>=36 (from Markdown==3.1.1->-r requirements-cpu.txt (line 9))\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/site-packages (from tensorboard==1.13.1->-r requirements-cpu.txt (line 16))\n",
      "Building wheels for collected packages: absl-py, gast, PyYAML, termcolor\n",
      "  Running setup.py bdist_wheel for absl-py: started\n",
      "  Running setup.py bdist_wheel for absl-py: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Running setup.py bdist_wheel for gast: started\n",
      "  Running setup.py bdist_wheel for gast: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Running setup.py bdist_wheel for PyYAML: started\n",
      "  Running setup.py bdist_wheel for PyYAML: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
      "  Running setup.py bdist_wheel for termcolor: started\n",
      "  Running setup.py bdist_wheel for termcolor: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built absl-py gast PyYAML termcolor\n",
      "Installing collected packages: six, absl-py, astor, gast, grpcio, numpy, h5py, PyYAML, Keras-Applications, Keras-Preprocessing, scipy, Keras, setuptools, Markdown, mock, protobuf, Werkzeug, tensorboard, tensorflow-estimator, termcolor, tensorflow\n",
      "  Found existing installation: setuptools 28.8.0\n",
      "    Uninstalling setuptools-28.8.0:\n",
      "      Successfully uninstalled setuptools-28.8.0\n",
      "Successfully installed Keras-2.2.4 Keras-Applications-1.0.8 Keras-Preprocessing-1.1.0 Markdown-3.1.1 PyYAML-5.1.1 Werkzeug-0.15.4 absl-py-0.7.1 astor-0.8.0 gast-0.2.2 grpcio-1.21.1 h5py-2.9.0 mock-3.0.5 numpy-1.16.4 protobuf-3.8.0 scipy-1.3.0 setuptools-41.0.1 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n",
      "\u001b[91mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 34cddd85d6f6\n",
      " ---> 51c5932bc0ab\n",
      "Step 5/6 : COPY src /opt/program\n",
      " ---> 50a2c12fc8f7\n",
      "Step 6/6 : WORKDIR /opt/program\n",
      " ---> Running in ac25d42c9bc0\n",
      "Removing intermediate container ac25d42c9bc0\n",
      " ---> 95c9b3aa81f0\n",
      "Successfully built 95c9b3aa81f0\n",
      "Successfully tagged keras-sagemaker-train:latest\n",
      "The push refers to repository [850021735523.dkr.ecr.us-east-1.amazonaws.com/keras-sagemaker-train]\n",
      "9f12d555da42: Preparing\n",
      "236bbdb0679d: Preparing\n",
      "95c677b3c980: Preparing\n",
      "952e0784686f: Preparing\n",
      "65c06ae44bbd: Preparing\n",
      "f194f1dd3e8f: Preparing\n",
      "ea264623c568: Preparing\n",
      "c4cd48200f79: Preparing\n",
      "bcc97fbfc9e1: Preparing\n",
      "f194f1dd3e8f: Waiting\n",
      "ea264623c568: Waiting\n",
      "c4cd48200f79: Waiting\n",
      "bcc97fbfc9e1: Waiting\n",
      "65c06ae44bbd: Layer already exists\n",
      "952e0784686f: Layer already exists\n",
      "f194f1dd3e8f: Layer already exists\n",
      "ea264623c568: Layer already exists\n",
      "c4cd48200f79: Layer already exists\n",
      "bcc97fbfc9e1: Layer already exists\n",
      "95c677b3c980: Pushed\n",
      "9f12d555da42: Pushed\n",
      "236bbdb0679d: Pushed\n",
      "latest: digest: sha256:cd0964d1cc64d93e4cd684d2936110b1e270c4b6272b5619b6da11d05b51548c size: 2213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=keras-sagemaker-train\n",
    "\n",
    "chmod +x src/*\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "# Comment the line below to use a GPU\n",
    "docker build  -t ${algorithm_name} -f Dockerfile.cpu .\n",
    "\n",
    "# Uncomment the below line if you wish to run on a GPU\n",
    "#docker build  -t ${algorithm_name} -f Dockerfile.gpu . \n",
    "\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 3: Define variables with data location and output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data location - s3://keras-sagemaker-train/data\n",
      "output location - s3://keras-sagemaker-train/output\n"
     ]
    }
   ],
   "source": [
    "data_location = 's3://{}/data'.format(bucket)\n",
    "print(\"data location - \" + data_location)\n",
    "\n",
    "output_location = 's3://{}/output'.format(bucket)\n",
    "print(\"output location - \" + output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4: Create a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 5: Define variables for account, region and algorithm image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account'] # aws account \n",
    "region = sess.boto_session.region_name # aws server region\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/keras-sagemaker-train'.format(account, region) # algorithm image path in ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 6: Define hyperparameters to be passed to your algorithm. \n",
    "In this project we are reading two hyperparameters for training. Use of hyperparameters in optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"batch_size\":128, \"epochs\":30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 7: Create the training job using SageMaker Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sage.estimator.Estimator(image_name=image, \n",
    "                                      role=role,\n",
    "                                      train_instance_count=1, \n",
    "                                      train_instance_type='ml.c5.2xlarge',\n",
    "                                      hyperparameters=hyperparameters,\n",
    "                                      output_path=output_location,\n",
    "                                      sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 8: Run the training job by passing the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-14 09:15:48 Starting - Starting the training job...\n",
      "2019-06-14 09:15:51 Starting - Launching requested ML instances......\n",
      "2019-06-14 09:17:02 Starting - Preparing the instances for training...\n",
      "2019-06-14 09:17:43 Downloading - Downloading input data\n",
      "2019-06-14 09:17:43 Training - Downloading the training image....\n",
      "\u001b[31m2019-06-14 09:18:20.768618: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[31m2019-06-14 09:18:20.799047: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\u001b[0m\n",
      "\u001b[31m2019-06-14 09:18:20.800177: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4826440 executing computations on platform Host. Devices:\u001b[0m\n",
      "\u001b[31m2019-06-14 09:18:20.800196: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\u001b[0m\n",
      "\u001b[31mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mColocations handled automatically by placer.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse tf.cast instead.\u001b[0m\n",
      "\u001b[31m[name: \"/device:CPU:0\"\u001b[0m\n",
      "\u001b[31mdevice_type: \"CPU\"\u001b[0m\n",
      "\u001b[31mmemory_limit: 268435456\u001b[0m\n",
      "\u001b[31mlocality {\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31mincarnation: 10817783467287168367\u001b[0m\n",
      "\u001b[31m, name: \"/device:XLA_CPU:0\"\u001b[0m\n",
      "\u001b[31mdevice_type: \"XLA_CPU\"\u001b[0m\n",
      "\u001b[31mmemory_limit: 17179869184\u001b[0m\n",
      "\u001b[31mlocality {\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31mincarnation: 5644020712441572999\u001b[0m\n",
      "\u001b[31mphysical_device_desc: \"device: XLA_CPU device\"\u001b[0m\n",
      "\u001b[31m]\u001b[0m\n",
      "\u001b[31mScript Status - Starting\u001b[0m\n",
      "\u001b[31mReading hyperparameters\u001b[0m\n",
      "\u001b[31mNumber of data samples:  10000\u001b[0m\n",
      "\u001b[31mNumber of data labels:  10000\u001b[0m\n",
      "\u001b[31mNumber of training samples:  ((8000, 784), (8000, 10))\u001b[0m\n",
      "\u001b[31mNumber of test samples:  ((2000, 784), (2000, 10))\u001b[0m\n",
      "\u001b[31mFinished reading the data.\u001b[0m\n",
      "\u001b[31mStarting the model training\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31mdense_1 (Dense)              (None, 512)               401920    \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdropout_1 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_2 (Dense)              (None, 512)               262656    \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdropout_2 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_3 (Dense)              (None, 10)                5130      \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31mTotal params: 669,706\u001b[0m\n",
      "\u001b[31mTrainable params: 669,706\u001b[0m\n",
      "\u001b[31mNon-trainable params: 0\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mTrain on 8000 samples, validate on 2000 samples\u001b[0m\n",
      "\u001b[31mEpoch 1/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 11s - loss: 2.3026 - acc: 0.1406\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 1s - loss: 2.2931 - acc: 0.1250 \u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 2.2717 - acc: 0.2142\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 2.2324 - acc: 0.2988\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 2.1763 - acc: 0.3529\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 2.1140 - acc: 0.3865\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 2.0513 - acc: 0.4138\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 1.9801 - acc: 0.4401\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 1s 80us/step - loss: 1.9405 - acc: 0.4516 - val_loss: 1.3897 - val_acc: 0.5535\u001b[0m\n",
      "\u001b[31mEpoch 2/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.4391 - acc: 0.5859\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 1.3427 - acc: 0.6224\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 1.3026 - acc: 0.6159\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 1.2588 - acc: 0.6292\u001b[0m\n",
      "\u001b[31m4480/8000 [===============>..............] - ETA: 0s - loss: 1.2195 - acc: 0.6400\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 1.1803 - acc: 0.6527\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 1.1452 - acc: 0.6606\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 1.1195 - acc: 0.6654\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 53us/step - loss: 1.1113 - acc: 0.6663 - val_loss: 0.8063 - val_acc: 0.7620\u001b[0m\n",
      "\u001b[31mEpoch 3/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.7680 - acc: 0.7891\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.8732 - acc: 0.7326\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.8785 - acc: 0.7256\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.8571 - acc: 0.7338\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.8351 - acc: 0.7398\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.8241 - acc: 0.7445\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.8191 - acc: 0.7460\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.8082 - acc: 0.7503\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.8063 - acc: 0.7505 - val_loss: 0.5954 - val_acc: 0.8455\u001b[0m\n",
      "\u001b[31mEpoch 4/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.7852 - acc: 0.7578\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.7240 - acc: 0.7734\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.7006 - acc: 0.7812\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.7040 - acc: 0.7777\u001b[0m\n",
      "\u001b[31m4096/8000 [==============>...............] - ETA: 0s - loss: 0.6912 - acc: 0.7800\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.6843 - acc: 0.7849\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.6776 - acc: 0.7876\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.6682 - acc: 0.7932\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.6666 - acc: 0.7945 - val_loss: 0.5357 - val_acc: 0.8435\u001b[0m\n",
      "\u001b[31mEpoch 5/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.6884 - acc: 0.7734\u001b[0m\n",
      "\u001b[31m1280/8000 [===>..........................] - ETA: 0s - loss: 0.6098 - acc: 0.8078\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.5891 - acc: 0.8164\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 0.5886 - acc: 0.8182\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 0.5845 - acc: 0.8185\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.5824 - acc: 0.8172\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.5743 - acc: 0.8203\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.5724 - acc: 0.8199\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.5712 - acc: 0.8195 - val_loss: 0.4208 - val_acc: 0.8870\u001b[0m\n",
      "\u001b[31mEpoch 6/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4146 - acc: 0.8828\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.4925 - acc: 0.8429\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.5007 - acc: 0.8456\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.5128 - acc: 0.8394\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 0.5092 - acc: 0.8394\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.5134 - acc: 0.8436\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.5102 - acc: 0.8416\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.5038 - acc: 0.8442\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 53us/step - loss: 0.5012 - acc: 0.8443 - val_loss: 0.3915 - val_acc: 0.8955\u001b[0m\n",
      "\u001b[31mEpoch 7/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.5258 - acc: 0.8281\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.4668 - acc: 0.8594\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.4731 - acc: 0.8598\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 0.4594 - acc: 0.8627\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 0.4600 - acc: 0.8582\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.4582 - acc: 0.8581\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.4531 - acc: 0.8602\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.4582 - acc: 0.8580\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.4550 - acc: 0.8596 - val_loss: 0.3337 - val_acc: 0.9160\u001b[0m\n",
      "\u001b[31mEpoch 8/30\n",
      "\u001b[0m\n",
      "\u001b[31m 128/8000 [..............................] - ETA: 0s - loss: 0.4315 - acc: 0.8984\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.4328 - acc: 0.8793\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.4285 - acc: 0.8713\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.4239 - acc: 0.8734\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.4287 - acc: 0.8714\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 0.4338 - acc: 0.8709\u001b[0m\n",
      "\u001b[31m6144/8000 [======================>.......] - ETA: 0s - loss: 0.4300 - acc: 0.8709\u001b[0m\n",
      "\u001b[31m7168/8000 [=========================>....] - ETA: 0s - loss: 0.4230 - acc: 0.8715\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 56us/step - loss: 0.4203 - acc: 0.8725 - val_loss: 0.3009 - val_acc: 0.9200\u001b[0m\n",
      "\u001b[31mEpoch 9/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4922 - acc: 0.8281\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.4144 - acc: 0.8759\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.3999 - acc: 0.8789\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 0.3954 - acc: 0.8804\u001b[0m\n",
      "\u001b[31m4480/8000 [===============>..............] - ETA: 0s - loss: 0.3910 - acc: 0.8810\u001b[0m\n",
      "\u001b[31m5504/8000 [===================>..........] - ETA: 0s - loss: 0.3935 - acc: 0.8775\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.3905 - acc: 0.8794\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.3889 - acc: 0.8797\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.3894 - acc: 0.8801 - val_loss: 0.3158 - val_acc: 0.9115\u001b[0m\n",
      "\u001b[31mEpoch 10/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4319 - acc: 0.8594\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3878 - acc: 0.8863\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.3564 - acc: 0.8925\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.3672 - acc: 0.8906\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.3611 - acc: 0.8913\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.3669 - acc: 0.8893\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.3677 - acc: 0.8887\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.3703 - acc: 0.8882\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.3685 - acc: 0.8890 - val_loss: 0.2581 - val_acc: 0.9335\u001b[0m\n",
      "\u001b[31mEpoch 11/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4745 - acc: 0.8828\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3873 - acc: 0.8898\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.3640 - acc: 0.8934\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.3549 - acc: 0.8928\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.3499 - acc: 0.8930\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.3489 - acc: 0.8931\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.3516 - acc: 0.8921\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.3521 - acc: 0.8904\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.3467 - acc: 0.8924 - val_loss: 0.2486 - val_acc: 0.9355\u001b[0m\n",
      "\u001b[31mEpoch 12/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2937 - acc: 0.9141\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3786 - acc: 0.8880\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.3462 - acc: 0.8961\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.3456 - acc: 0.8972\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 0.3440 - acc: 0.8955\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.3328 - acc: 0.8988\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.3345 - acc: 0.8988\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.3256 - acc: 0.9015\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 53us/step - loss: 0.3259 - acc: 0.9005 - val_loss: 0.2471 - val_acc: 0.9355\u001b[0m\n",
      "\u001b[31mEpoch 13/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2918 - acc: 0.8984\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3260 - acc: 0.8967\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.3177 - acc: 0.8966\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.3173 - acc: 0.8972\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.3159 - acc: 0.9015\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.3170 - acc: 0.9015\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.3098 - acc: 0.9043\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.3167 - acc: 0.9009\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 55us/step - loss: 0.3117 - acc: 0.9031 - val_loss: 0.2420 - val_acc: 0.9305\u001b[0m\n",
      "\u001b[31mEpoch 14/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2113 - acc: 0.9375\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3050 - acc: 0.9089\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.3025 - acc: 0.9097\u001b[0m\n",
      "\u001b[31m3456/8000 [===========>..................] - ETA: 0s - loss: 0.3100 - acc: 0.9054\u001b[0m\n",
      "\u001b[31m4480/8000 [===============>..............] - ETA: 0s - loss: 0.2971 - acc: 0.9080\u001b[0m\n",
      "\u001b[31m5504/8000 [===================>..........] - ETA: 0s - loss: 0.3019 - acc: 0.9059\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.2993 - acc: 0.9073\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2961 - acc: 0.9082\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.2942 - acc: 0.9089 - val_loss: 0.2195 - val_acc: 0.9370\u001b[0m\n",
      "\u001b[31mEpoch 15/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2716 - acc: 0.9141\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2735 - acc: 0.9141\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2747 - acc: 0.9159\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2687 - acc: 0.9175\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.2748 - acc: 0.9167\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.2705 - acc: 0.9163\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.2774 - acc: 0.9137\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.2803 - acc: 0.9116\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.2826 - acc: 0.9106 - val_loss: 0.2117 - val_acc: 0.9415\u001b[0m\n",
      "\u001b[31mEpoch 16/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3009 - acc: 0.8984\u001b[0m\n",
      "\u001b[31m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2642 - acc: 0.9156\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.2703 - acc: 0.9123\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 0.2613 - acc: 0.9159\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 0.2640 - acc: 0.9159\u001b[0m\n",
      "\u001b[31m5504/8000 [===================>..........] - ETA: 0s - loss: 0.2588 - acc: 0.9161\u001b[0m\n",
      "\u001b[31m6528/8000 [=======================>......] - ETA: 0s - loss: 0.2569 - acc: 0.9176\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2650 - acc: 0.9155\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 53us/step - loss: 0.2677 - acc: 0.9147 - val_loss: 0.2267 - val_acc: 0.9345\u001b[0m\n",
      "\u001b[31mEpoch 17/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2345 - acc: 0.9375\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2756 - acc: 0.9227\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2623 - acc: 0.9228\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2697 - acc: 0.9187\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.2632 - acc: 0.9202\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.2625 - acc: 0.9198\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.2543 - acc: 0.9222\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.2558 - acc: 0.9217\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.2585 - acc: 0.9211 - val_loss: 0.2036 - val_acc: 0.9440\u001b[0m\n",
      "\u001b[31mEpoch 18/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2124 - acc: 0.9219\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2375 - acc: 0.9271\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.2508 - acc: 0.9227\u001b[0m\n",
      "\u001b[31m3456/8000 [===========>..................] - ETA: 0s - loss: 0.2435 - acc: 0.9256\u001b[0m\n",
      "\u001b[31m4480/8000 [===============>..............] - ETA: 0s - loss: 0.2433 - acc: 0.9237\u001b[0m\n",
      "\u001b[31m5504/8000 [===================>..........] - ETA: 0s - loss: 0.2472 - acc: 0.9231\u001b[0m\n",
      "\u001b[31m6528/8000 [=======================>......] - ETA: 0s - loss: 0.2472 - acc: 0.9236\u001b[0m\n",
      "\u001b[31m7552/8000 [===========================>..] - ETA: 0s - loss: 0.2497 - acc: 0.9236\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.2484 - acc: 0.9240 - val_loss: 0.2065 - val_acc: 0.9420\u001b[0m\n",
      "\u001b[31mEpoch 19/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3698 - acc: 0.9297\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2087 - acc: 0.9453\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2251 - acc: 0.9357\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2263 - acc: 0.9325\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.2331 - acc: 0.9313\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.2326 - acc: 0.9297\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.2320 - acc: 0.9294\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.2319 - acc: 0.9294\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.2320 - acc: 0.9293 - val_loss: 0.1994 - val_acc: 0.9440\u001b[0m\n",
      "\u001b[31mEpoch 20/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3161 - acc: 0.8984\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2152 - acc: 0.9332\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.2241 - acc: 0.9307\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.2340 - acc: 0.9274\u001b[0m\n",
      "\u001b[31m4096/8000 [==============>...............] - ETA: 0s - loss: 0.2356 - acc: 0.9272\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 0.2284 - acc: 0.9295\u001b[0m\n",
      "\u001b[31m6144/8000 [======================>.......] - ETA: 0s - loss: 0.2234 - acc: 0.9303\u001b[0m\n",
      "\u001b[31m7168/8000 [=========================>....] - ETA: 0s - loss: 0.2227 - acc: 0.9314\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 55us/step - loss: 0.2243 - acc: 0.9303 - val_loss: 0.1990 - val_acc: 0.9430\u001b[0m\n",
      "\u001b[31mEpoch 21/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2905 - acc: 0.9141\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2068 - acc: 0.9375\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2284 - acc: 0.9292\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 0.2157 - acc: 0.9327\u001b[0m\n",
      "\u001b[31m4480/8000 [===============>..............] - ETA: 0s - loss: 0.2185 - acc: 0.9299\u001b[0m\n",
      "\u001b[31m5632/8000 [====================>.........] - ETA: 0s - loss: 0.2193 - acc: 0.9304\u001b[0m\n",
      "\u001b[31m6656/8000 [=======================>......] - ETA: 0s - loss: 0.2175 - acc: 0.9313\u001b[0m\n",
      "\u001b[31m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2241 - acc: 0.9290\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.2215 - acc: 0.9300 - val_loss: 0.1868 - val_acc: 0.9465\u001b[0m\n",
      "\u001b[31mEpoch 22/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2107 - acc: 0.9531\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1973 - acc: 0.9444\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2154 - acc: 0.9338\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2183 - acc: 0.9334\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.2208 - acc: 0.9325\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.2106 - acc: 0.9341\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.2080 - acc: 0.9351\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.2055 - acc: 0.9359\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.2091 - acc: 0.9346 - val_loss: 0.1818 - val_acc: 0.9470\u001b[0m\n",
      "\u001b[31mEpoch 23/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1902 - acc: 0.9375\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1928 - acc: 0.9392\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2012 - acc: 0.9370\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2038 - acc: 0.9353\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.1964 - acc: 0.9373\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.2004 - acc: 0.9362\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.1987 - acc: 0.9378\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.2024 - acc: 0.9382\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 55us/step - loss: 0.2014 - acc: 0.9382 - val_loss: 0.1766 - val_acc: 0.9510\u001b[0m\n",
      "\u001b[31mEpoch 24/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1680 - acc: 0.9453\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1932 - acc: 0.9436\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-06-14 09:18:46 Uploading - Uploading generated training model\n",
      "2019-06-14 09:18:46 Completed - Training job completed\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1839 - acc: 0.9481\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.1870 - acc: 0.9450\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.1805 - acc: 0.9465\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.1767 - acc: 0.9457\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.1872 - acc: 0.9424\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1882 - acc: 0.9417\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.1894 - acc: 0.9406 - val_loss: 0.1819 - val_acc: 0.9485\u001b[0m\n",
      "\u001b[31mEpoch 25/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2170 - acc: 0.9375\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1464 - acc: 0.9583\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1645 - acc: 0.9490\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.1723 - acc: 0.9434\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.1836 - acc: 0.9408\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.1875 - acc: 0.9399\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.1862 - acc: 0.9414\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1866 - acc: 0.9414\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 53us/step - loss: 0.1847 - acc: 0.9420 - val_loss: 0.1658 - val_acc: 0.9535\u001b[0m\n",
      "\u001b[31mEpoch 26/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.0881 - acc: 0.9688\u001b[0m\n",
      "\u001b[31m1280/8000 [===>..........................] - ETA: 0s - loss: 0.1911 - acc: 0.9398\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.1858 - acc: 0.9440\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 0.1827 - acc: 0.9426\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 0.1773 - acc: 0.9449\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.1720 - acc: 0.9459\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.1715 - acc: 0.9469\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1729 - acc: 0.9473\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.1743 - acc: 0.9461 - val_loss: 0.1667 - val_acc: 0.9510\u001b[0m\n",
      "\u001b[31mEpoch 27/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1441 - acc: 0.9531\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1517 - acc: 0.9540\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1728 - acc: 0.9467\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.1785 - acc: 0.9441\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.1798 - acc: 0.9446\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.1748 - acc: 0.9463\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.1703 - acc: 0.9477\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1680 - acc: 0.9490\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.1675 - acc: 0.9483 - val_loss: 0.1686 - val_acc: 0.9545\u001b[0m\n",
      "\u001b[31mEpoch 28/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.0929 - acc: 0.9766\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1259 - acc: 0.9661\u001b[0m\n",
      "\u001b[31m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1340 - acc: 0.9600\u001b[0m\n",
      "\u001b[31m3072/8000 [==========>...................] - ETA: 0s - loss: 0.1465 - acc: 0.9570\u001b[0m\n",
      "\u001b[31m4096/8000 [==============>...............] - ETA: 0s - loss: 0.1531 - acc: 0.9517\u001b[0m\n",
      "\u001b[31m5120/8000 [==================>...........] - ETA: 0s - loss: 0.1541 - acc: 0.9514\u001b[0m\n",
      "\u001b[31m6144/8000 [======================>.......] - ETA: 0s - loss: 0.1531 - acc: 0.9508\u001b[0m\n",
      "\u001b[31m7168/8000 [=========================>....] - ETA: 0s - loss: 0.1626 - acc: 0.9494\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 55us/step - loss: 0.1616 - acc: 0.9497 - val_loss: 0.1799 - val_acc: 0.9510\u001b[0m\n",
      "\u001b[31mEpoch 29/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1427 - acc: 0.9609\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1408 - acc: 0.9566\u001b[0m\n",
      "\u001b[31m2304/8000 [=======>......................] - ETA: 0s - loss: 0.1430 - acc: 0.9566\u001b[0m\n",
      "\u001b[31m3328/8000 [===========>..................] - ETA: 0s - loss: 0.1430 - acc: 0.9561\u001b[0m\n",
      "\u001b[31m4352/8000 [===============>..............] - ETA: 0s - loss: 0.1449 - acc: 0.9543\u001b[0m\n",
      "\u001b[31m5376/8000 [===================>..........] - ETA: 0s - loss: 0.1481 - acc: 0.9542\u001b[0m\n",
      "\u001b[31m6400/8000 [=======================>......] - ETA: 0s - loss: 0.1504 - acc: 0.9536\u001b[0m\n",
      "\u001b[31m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1547 - acc: 0.9519\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.1527 - acc: 0.9530 - val_loss: 0.1767 - val_acc: 0.9515\u001b[0m\n",
      "\u001b[31mEpoch 30/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1370 - acc: 0.9375\u001b[0m\n",
      "\u001b[31m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1254 - acc: 0.9557\u001b[0m\n",
      "\u001b[31m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1281 - acc: 0.9554\u001b[0m\n",
      "\u001b[31m3200/8000 [===========>..................] - ETA: 0s - loss: 0.1299 - acc: 0.9544\u001b[0m\n",
      "\u001b[31m4224/8000 [==============>...............] - ETA: 0s - loss: 0.1424 - acc: 0.9527\u001b[0m\n",
      "\u001b[31m5248/8000 [==================>...........] - ETA: 0s - loss: 0.1503 - acc: 0.9526\u001b[0m\n",
      "\u001b[31m6272/8000 [======================>.......] - ETA: 0s - loss: 0.1522 - acc: 0.9520\u001b[0m\n",
      "\u001b[31m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1510 - acc: 0.9531\u001b[0m\n",
      "\u001b[31m8000/8000 [==============================] - 0s 54us/step - loss: 0.1502 - acc: 0.9531 - val_loss: 0.1754 - val_acc: 0.9465\u001b[0m\n",
      "\u001b[31mTest loss: 0.17542570799589158\u001b[0m\n",
      "\u001b[31mTest accuracy: 0.9465\u001b[0m\n",
      "\u001b[31mFinished training the model.\u001b[0m\n",
      "\u001b[31mFinished training the model.\u001b[0m\n",
      "\u001b[31mScript Status - Finished\u001b[0m\n",
      "\u001b[31mTotal time taken to train the model:  18.998392343521118\u001b[0m\n",
      "Billable seconds: 74\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! We had a successful training job run in Amazon SageMaker.\n",
    "#### Please return to the tutorial for Part 6 where we will be running a training job in a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
